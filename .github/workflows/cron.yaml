# name: Run Twitter scraper hourly

# on:
#   schedule:
#     - cron: "*/45 * * * *"   # cada hora (UTC)
#   workflow_dispatch:

# permissions:
#   contents: write

# jobs:
#   scrape:
#     runs-on: ubuntu-latest
#     environment: prueba-Actions
#     concurrency:
#       group: hourly-scrape
#       cancel-in-progress: true

#     # ‚öôÔ∏è Config p√∫blica/no sensible directamente en YAML
#     env:
#       APIFY_ACTOR_ID: ${{ secrets.ACTOR_ID }}
#       SEARCH_TERMS: ${{ vars.SEARCH_TERMS }}

#     steps:
#       - name: Checkout repo
#         uses: actions/checkout@v4
#         with: { fetch-depth: 0 }

#       - name: Set up Python
#         uses: actions/setup-python@v5
#         with: { python-version: "3.11" }

#       - name: Cache pip
#         uses: actions/cache@v4
#         with:
#           path: ~/.cache/pip
#           key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
#           restore-keys: ${{ runner.os }}-pip-

#       - name: Install dependencies
#         run: |
#           python -m pip install --upgrade pip
#           pip install -r requirements.txt

#       - name: Run scraper
#         # üîê el token sigue viniendo de Secrets (m√°s seguro)
#         env:
#           APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
#         run: python script.py

#       - name: Upload CSV artifact
#         uses: actions/upload-artifact@v4
#         with:
#           name: twitter-scrape-output
#           path: output/*.csv
#           if-no-files-found: warn
#           retention-days: 30

#       - name: Commit CSV to repo
#         if: ${{ hashFiles('output/*.csv') != '' }}
#         uses: EndBug/add-and-commit@v9
#         with:
#           add: "output/*.csv"
#           message: "scrape: output ${{ github.run_id }}"
#           default_author: github_actions
